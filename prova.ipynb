{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sto usando cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\GRVRLD00P\\AppData\\Local\\Temp\\ipykernel_23544\\4126360923.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  descrizioni = torch.load(desc_path)\n",
      "c:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:473: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [2.2897],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [2.4953],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [1.8267],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan]], device='cuda:0')\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 155\u001b[0m\n\u001b[0;32m    153\u001b[0m datashuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocationId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datashuffle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocationId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m val \u001b[38;5;241m=\u001b[39m getDataset(references, data, descriptions, target, img_path, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m \u001b[43mvalidation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodello\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShuffle\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    157\u001b[0m val \u001b[38;5;241m=\u001b[39m getDataset(references, datashuffle, descriptions, target, img_path, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 73\u001b[0m, in \u001b[0;36mvalidation_loop\u001b[1;34m(dataloader, model, loss_fn, device)\u001b[0m\n\u001b[0;32m     71\u001b[0m img, desc, tab, y \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device), desc\u001b[38;5;241m.\u001b[39mto(device), tab\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(img, tab, desc)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m y_np, pred_np \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     75\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(y_np, label)\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    460\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    461\u001b[0m     )\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor_str.py:698\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    697\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor_str.py:618\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    616\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    617\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    621\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor_str.py:350\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 350\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import ImageReadMode\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import sales_prediction.sales_prediction as sp\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406) #presi da timm\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, references, tabular_data, descriptions, img_path, target, transform=None, target_transform=None):\n",
    "        self.img_ref = references \n",
    "        self.tabular = tabular_data\n",
    "        self.descriptions = descriptions\n",
    "        self.target = target\n",
    "        self.img_path = img_path\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ref)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.img_path + self.img_ref[idx], ImageReadMode.RGB)\n",
    "        \n",
    "        tabular_row = torch.from_numpy(self.tabular.iloc[idx].values).float()\n",
    "\n",
    "        desc_tensor = self.descriptions[self.img_ref[idx]]\n",
    "        \n",
    "        target = self.target[idx]\n",
    "        \n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return image, tabular_row, desc_tensor, target \n",
    "\n",
    "def getDataset(references, tabular_data, descriptions, target, img_path, batch_size, proportion):\n",
    "    transform_img = v2.Compose([\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "    ])\n",
    "    \n",
    "    dataset = SalesDataset(references, tabular_data, descriptions, img_path, target, transform_img, None)\n",
    "    \n",
    "    dataset, _ = random_split(dataset, [proportion, 1 - proportion])\n",
    "\n",
    "    validation_dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    return validation_dataloader\n",
    "\n",
    "def validation_loop(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    mae = nn.L1Loss()\n",
    "    avg_mse, avg_mae = 0, 0\n",
    "    label, prediction = np.array([]), np.array([])\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for img, tab, desc, y in dataloader:\n",
    "            img, desc, tab, y = img.to(device), desc.to(device), tab.to(device), y.to(device)\n",
    "            pred = model(img, tab, desc)\n",
    "            print(pred)\n",
    "            y_np, pred_np = y.cpu().detach().numpy(), pred.cpu().detach().numpy()\n",
    "            label = np.append(y_np, label)\n",
    "            prediction = np.append(pred_np, prediction)\n",
    "            \n",
    "            avg_mse += loss_fn(pred.squeeze(), y.float()).item()\n",
    "            avg_mae += mae(pred.squeeze(), y.float()).item()\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print(i)\n",
    "            i= i+1\n",
    "            \n",
    "    avg_mse /= num_batches\n",
    "    avg_mae /= num_batches\n",
    "    r2 = r2_score(label, prediction)\n",
    "    bias = np.mean(prediction - label)\n",
    "    \n",
    "    print(f\"Validation Error: \\n Avg MSE: {avg_mse:>8f} \\n Avg MAE: {avg_mae:>8f} \\n R2: {r2:>8f}\\n\" + \n",
    "          f\" Bias: {bias:>8f}\\n\")\n",
    "    \n",
    "    return avg_mse, avg_mae, r2, bias\n",
    "\n",
    "def get_tabular(tabular_path, desc_path):\n",
    "    data, references, target = get_data(tabular_path)\n",
    "    #tokenized_desc = word_embedding(descriptions)\n",
    "    descrizioni = torch.load(desc_path)\n",
    "    return data, references, descrizioni, target\n",
    "\n",
    "def get_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.sample(8000)\n",
    "    references = data['IdProdotto'].values\n",
    "    target = data['Quantity'].values\n",
    "    \n",
    "    data = data.drop(columns = ['Descrizione', 'IdProdotto', 'Quantity'], axis='columns')\n",
    "  \n",
    "    columns = ['CodiceColore', 'PianoTaglia', 'WaveCode', 'AstronomicalSeasonExternalID', 'SalesSeasonDescription']\n",
    "    for col in columns:\n",
    "        encoded_labels, _ = pd.factorize(data[col])\n",
    "        data[col] = encoded_labels\n",
    "    \n",
    "    for col in data.columns: #normalizzo tutto tranne la quantit√†\n",
    "        if col != 'Quantity':\n",
    "            val = data[col]\n",
    "            if val.std() != 0:\n",
    "                normalized_labels = (val - val.mean())/val.std()\n",
    "                data[col] = normalized_labels\n",
    "\n",
    "    return data, references, target\n",
    "\n",
    "img_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\ResizedImages\\\\'\n",
    "desc_path= 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\descrizioni\\\\descrizioni.pt'\n",
    "desc_tot_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\descrizioni\\\\descrizioni_tot.pt'\n",
    "\n",
    "data_week = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_week.csv'\n",
    "data_month = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_month.csv'\n",
    "data_season = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_season.csv'\n",
    "\n",
    "noneg_week = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\nonegozio_week.csv'\n",
    "noneg_month = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\nonegozio_month.csv'\n",
    "noneg_season = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\nonegozio_season.csv'\n",
    "\n",
    "path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\results\\\\month_negozi\\\\weights.pt'\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Sto usando {device}\")\n",
    "\n",
    "modello = sp.create_model(0,1)\n",
    "modello = torch.jit.load(path)\n",
    "modello.eval()\n",
    "\n",
    "data, references, descriptions, target = get_tabular(data_month, desc_path)\n",
    "datashuffle = data\n",
    "datashuffle['LocationId'] = datashuffle['LocationId'].sample(frac=1).reset_index(drop=True)\n",
    "val = getDataset(references, data, descriptions, target, img_path, 128, 1)\n",
    "validation_loop(val, modello, nn.MSELoss(), device)\n",
    "print('Shuffle\\n')\n",
    "val = getDataset(references, datashuffle, descriptions, target, img_path, 128, 1)\n",
    "validation_loop(val, modello, nn.MSELoss(), device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
