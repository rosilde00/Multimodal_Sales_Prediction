{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sto usando cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\GRVRLD00P\\Documents\\Progetto ORS\\Progetto ORS\\ProgettoORS\\sales_prediction\\preproc_tabular.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  descrizioni = torch.load(desc_path)\n",
      "c:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:473: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodello\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     mse, mae, r2, bias \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mvalidation_loop(val, modello, criterion, device)\n\u001b[0;32m     54\u001b[0m     early_stp \u001b[38;5;241m=\u001b[39m early_stop(mse)\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\Documents\\Progetto ORS\\Progetto ORS\\ProgettoORS\\sales_prediction\\step\\pred_imgtab.py:110\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, batch_size, device)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (img, tab, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    108\u001b[0m     img, tab, y \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device), tab\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 110\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39msqueeze(), y\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\Documents\\Progetto ORS\\Progetto ORS\\ProgettoORS\\sales_prediction\\step\\pred_imgtab.py:69\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[1;34m(self, image, tab)\u001b[0m\n\u001b[0;32m     65\u001b[0m emb_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage(vit)\n\u001b[0;32m     67\u001b[0m emb_tab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtabular(tab)\n\u001b[1;32m---> 69\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_tab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sales_prediction.step.pred_imgtab as pt\n",
    "from sales_prediction.sales_prediction import EarlyStopping\n",
    "from sales_prediction.preproc_tabular import get_tabular\n",
    "from torch.optim.adamw import AdamW\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "img_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\ResizedImages\\\\'\n",
    "desc_path= 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\descrizioni\\\\descrizioni.pt'\n",
    "desc_tot_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\descrizioni\\\\descrizioni_tot.pt'\n",
    "\n",
    "tab_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\sales_anagrafica_final.csv'\n",
    "aggregated_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_season.csv'\n",
    "aggregated_month_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\aggregated_sales_month.csv'\n",
    "\n",
    "noneg_week = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\sales_anagrafica_final.csv'\n",
    "noneg_month = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_season.csv'\n",
    "noneg_season = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\nonegozio_season.csv'\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Sto usando {device}\")\n",
    "\n",
    "modello = pt.create_model(1,1).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": modello.image.parameters(), \"lr\": 1e-2},\n",
    "        {\"params\": modello.tabular.parameters()},\n",
    "        {\"params\": modello.final.parameters()}\n",
    "    ],\n",
    "    lr=1e-1,\n",
    ") \n",
    "early_stop = EarlyStopping(7, 0.001) \n",
    "\n",
    "data, references, _, target = get_tabular(aggregated_path, desc_path)\n",
    "\n",
    "train, val = pt.getDataset(references, data, target, img_path, batch_size, 1)\n",
    "\n",
    "mse_final, mae_final, r2_final = 0, 0, 0\n",
    "modello_final = 0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    pt.train_loop(train, modello, criterion, optimizer, batch_size, device)\n",
    "    mse, mae, r2, bias = pt.validation_loop(val, modello, criterion, device)\n",
    "    early_stp = early_stop(mse)\n",
    "\n",
    "    if early_stp:\n",
    "        print('Early Stop attivato.')\n",
    "        break\n",
    "    \n",
    "print(\"Fatto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sto usando cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GRVRLD00P\\Documents\\Progetto ORS\\Progetto ORS\\ProgettoORS\\sales_prediction\\preproc_tabular.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  descrizioni = torch.load(desc_path)\n",
      "c:\\Users\\GRVRLD00P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:473: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "MSE: 268.343262  [   64/58532]\n",
      "MSE: 139.866364  [ 6464/58532]\n",
      "MSE: 246.485382  [12864/58532]\n",
      "MSE: 222.235718  [19264/58532]\n",
      "MSE: 269.551514  [25664/58532]\n",
      "MSE: 128.585129  [32064/58532]\n",
      "MSE: 156.166016  [38464/58532]\n",
      "MSE: 104.648170  [44864/58532]\n",
      "MSE: 124.745071  [51264/58532]\n",
      "MSE: 79.374199  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 168.53939819335938\n",
      "Validation Error: \n",
      " Avg MSE: 111.652055 \n",
      " Avg MAE: 7.353706 \n",
      " Avg R2: -0.284939\n",
      " Bias: -5.914383\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "MSE: 102.448685  [   64/58532]\n",
      "MSE: 53.291706  [ 6464/58532]\n",
      "MSE: 130.123260  [12864/58532]\n",
      "MSE: 122.434525  [19264/58532]\n",
      "MSE: 160.459198  [25664/58532]\n",
      "MSE: 67.932022  [32064/58532]\n",
      "MSE: 94.006729  [38464/58532]\n",
      "MSE: 74.335327  [44864/58532]\n",
      "MSE: 83.344994  [51264/58532]\n",
      "MSE: 49.574680  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 93.00325775146484\n",
      "Validation Error: \n",
      " Avg MSE: 81.469113 \n",
      " Avg MAE: 6.314607 \n",
      " Avg R2: 0.075862\n",
      " Bias: -2.240187\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "MSE: 70.240906  [   64/58532]\n",
      "MSE: 44.014885  [ 6464/58532]\n",
      "MSE: 103.724075  [12864/58532]\n",
      "MSE: 100.275436  [19264/58532]\n",
      "MSE: 138.193542  [25664/58532]\n",
      "MSE: 59.999290  [32064/58532]\n",
      "MSE: 85.194794  [38464/58532]\n",
      "MSE: 73.332657  [44864/58532]\n",
      "MSE: 78.102882  [51264/58532]\n",
      "MSE: 45.741962  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 79.18598175048828\n",
      "Validation Error: \n",
      " Avg MSE: 76.987307 \n",
      " Avg MAE: 6.332241 \n",
      " Avg R2: 0.125047\n",
      " Bias: -0.901550\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "MSE: 65.911446  [   64/58532]\n",
      "MSE: 46.267952  [ 6464/58532]\n",
      "MSE: 100.447189  [12864/58532]\n",
      "MSE: 96.458412  [19264/58532]\n",
      "MSE: 127.769302  [25664/58532]\n",
      "MSE: 59.457855  [32064/58532]\n",
      "MSE: 82.734688  [38464/58532]\n",
      "MSE: 70.878456  [44864/58532]\n",
      "MSE: 76.830109  [51264/58532]\n",
      "MSE: 41.536831  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 76.78814697265625\n",
      "Validation Error: \n",
      " Avg MSE: 74.913072 \n",
      " Avg MAE: 6.340792 \n",
      " Avg R2: 0.146636\n",
      " Bias: -0.305660\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "MSE: 57.949463  [   64/58532]\n",
      "MSE: 42.124939  [ 6464/58532]\n",
      "MSE: 92.048477  [12864/58532]\n",
      "MSE: 93.836357  [19264/58532]\n",
      "MSE: 124.137642  [25664/58532]\n",
      "MSE: 59.556778  [32064/58532]\n",
      "MSE: 83.624519  [38464/58532]\n",
      "MSE: 71.953438  [44864/58532]\n",
      "MSE: 74.317986  [51264/58532]\n",
      "MSE: 40.614632  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.84386444091797\n",
      "Validation Error: \n",
      " Avg MSE: 74.533359 \n",
      " Avg MAE: 6.346774 \n",
      " Avg R2: 0.149652\n",
      " Bias: -0.180158\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "MSE: 58.211914  [   64/58532]\n",
      "MSE: 43.591190  [ 6464/58532]\n",
      "MSE: 91.064636  [12864/58532]\n",
      "MSE: 93.492889  [19264/58532]\n",
      "MSE: 122.872208  [25664/58532]\n",
      "MSE: 60.427322  [32064/58532]\n",
      "MSE: 83.576164  [38464/58532]\n",
      "MSE: 71.988770  [44864/58532]\n",
      "MSE: 74.310287  [51264/58532]\n",
      "MSE: 42.207535  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.4789810180664\n",
      "Validation Error: \n",
      " Avg MSE: 74.416142 \n",
      " Avg MAE: 6.341751 \n",
      " Avg R2: 0.150952\n",
      " Bias: -0.208537\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "MSE: 57.675026  [   64/58532]\n",
      "MSE: 42.786957  [ 6464/58532]\n",
      "MSE: 90.527527  [12864/58532]\n",
      "MSE: 93.283669  [19264/58532]\n",
      "MSE: 122.311371  [25664/58532]\n",
      "MSE: 60.260132  [32064/58532]\n",
      "MSE: 83.070915  [38464/58532]\n",
      "MSE: 72.229210  [44864/58532]\n",
      "MSE: 73.575188  [51264/58532]\n",
      "MSE: 42.118481  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.4038314819336\n",
      "Validation Error: \n",
      " Avg MSE: 74.236717 \n",
      " Avg MAE: 6.344875 \n",
      " Avg R2: 0.152561\n",
      " Bias: -0.122359\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "MSE: 57.734341  [   64/58532]\n",
      "MSE: 42.807156  [ 6464/58532]\n",
      "MSE: 89.876945  [12864/58532]\n",
      "MSE: 94.342896  [19264/58532]\n",
      "MSE: 121.973648  [25664/58532]\n",
      "MSE: 60.280411  [32064/58532]\n",
      "MSE: 83.826637  [38464/58532]\n",
      "MSE: 72.148148  [44864/58532]\n",
      "MSE: 73.114937  [51264/58532]\n",
      "MSE: 40.700157  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.37107849121094\n",
      "Validation Error: \n",
      " Avg MSE: 74.309535 \n",
      " Avg MAE: 6.341454 \n",
      " Avg R2: 0.152153\n",
      " Bias: -0.147101\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "MSE: 58.438290  [   64/58532]\n",
      "MSE: 42.286160  [ 6464/58532]\n",
      "MSE: 90.977631  [12864/58532]\n",
      "MSE: 93.973473  [19264/58532]\n",
      "MSE: 122.046043  [25664/58532]\n",
      "MSE: 60.040672  [32064/58532]\n",
      "MSE: 84.051468  [38464/58532]\n",
      "MSE: 72.270218  [44864/58532]\n",
      "MSE: 73.415604  [51264/58532]\n",
      "MSE: 40.603741  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.34457397460938\n",
      "Validation Error: \n",
      " Avg MSE: 74.301099 \n",
      " Avg MAE: 6.347140 \n",
      " Avg R2: 0.151812\n",
      " Bias: -0.102918\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "MSE: 58.561546  [   64/58532]\n",
      "MSE: 42.108063  [ 6464/58532]\n",
      "MSE: 89.745361  [12864/58532]\n",
      "MSE: 94.543999  [19264/58532]\n",
      "MSE: 121.039650  [25664/58532]\n",
      "MSE: 60.109955  [32064/58532]\n",
      "MSE: 83.603989  [38464/58532]\n",
      "MSE: 72.498688  [44864/58532]\n",
      "MSE: 73.601990  [51264/58532]\n",
      "MSE: 40.579559  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.33799743652344\n",
      "Validation Error: \n",
      " Avg MSE: 74.248277 \n",
      " Avg MAE: 6.336329 \n",
      " Avg R2: 0.152867\n",
      " Bias: -0.162914\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "MSE: 58.359135  [   64/58532]\n",
      "MSE: 42.511635  [ 6464/58532]\n",
      "MSE: 92.468018  [12864/58532]\n",
      "MSE: 94.507248  [19264/58532]\n",
      "MSE: 121.949493  [25664/58532]\n",
      "MSE: 60.128242  [32064/58532]\n",
      "MSE: 84.000381  [38464/58532]\n",
      "MSE: 72.114799  [44864/58532]\n",
      "MSE: 73.832504  [51264/58532]\n",
      "MSE: 40.495850  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.34832763671875\n",
      "Validation Error: \n",
      " Avg MSE: 74.233147 \n",
      " Avg MAE: 6.340159 \n",
      " Avg R2: 0.152816\n",
      " Bias: -0.141164\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "MSE: 58.155556  [   64/58532]\n",
      "MSE: 42.040245  [ 6464/58532]\n",
      "MSE: 91.575119  [12864/58532]\n",
      "MSE: 93.836327  [19264/58532]\n",
      "MSE: 121.684456  [25664/58532]\n",
      "MSE: 60.235168  [32064/58532]\n",
      "MSE: 83.861702  [38464/58532]\n",
      "MSE: 72.294609  [44864/58532]\n",
      "MSE: 73.430298  [51264/58532]\n",
      "MSE: 43.196796  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.30317687988281\n",
      "Validation Error: \n",
      " Avg MSE: 74.147935 \n",
      " Avg MAE: 6.346067 \n",
      " Avg R2: 0.153310\n",
      " Bias: -0.100054\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "MSE: 58.173042  [   64/58532]\n",
      "MSE: 42.198723  [ 6464/58532]\n",
      "MSE: 89.342285  [12864/58532]\n",
      "MSE: 93.883163  [19264/58532]\n",
      "MSE: 121.805771  [25664/58532]\n",
      "MSE: 60.086655  [32064/58532]\n",
      "MSE: 83.368607  [38464/58532]\n",
      "MSE: 72.147377  [44864/58532]\n",
      "MSE: 73.386063  [51264/58532]\n",
      "MSE: 40.407104  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.29591369628906\n",
      "Validation Error: \n",
      " Avg MSE: 74.303212 \n",
      " Avg MAE: 6.343104 \n",
      " Avg R2: 0.152208\n",
      " Bias: -0.117724\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "MSE: 58.261341  [   64/58532]\n",
      "MSE: 42.142544  [ 6464/58532]\n",
      "MSE: 89.594002  [12864/58532]\n",
      "MSE: 95.804001  [19264/58532]\n",
      "MSE: 121.824966  [25664/58532]\n",
      "MSE: 60.191254  [32064/58532]\n",
      "MSE: 83.991394  [38464/58532]\n",
      "MSE: 72.618858  [44864/58532]\n",
      "MSE: 73.679749  [51264/58532]\n",
      "MSE: 40.683296  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.3267822265625\n",
      "Validation Error: \n",
      " Avg MSE: 74.195992 \n",
      " Avg MAE: 6.342667 \n",
      " Avg R2: 0.153026\n",
      " Bias: -0.122716\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "MSE: 58.017471  [   64/58532]\n",
      "MSE: 42.457207  [ 6464/58532]\n",
      "MSE: 95.294342  [12864/58532]\n",
      "MSE: 94.670578  [19264/58532]\n",
      "MSE: 121.894432  [25664/58532]\n",
      "MSE: 60.567215  [32064/58532]\n",
      "MSE: 82.956581  [38464/58532]\n",
      "MSE: 72.549088  [44864/58532]\n",
      "MSE: 73.764030  [51264/58532]\n",
      "MSE: 40.267189  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.2666015625\n",
      "Validation Error: \n",
      " Avg MSE: 74.231451 \n",
      " Avg MAE: 6.329232 \n",
      " Avg R2: 0.153329\n",
      " Bias: -0.185967\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "MSE: 58.306763  [   64/58532]\n",
      "MSE: 42.111015  [ 6464/58532]\n",
      "MSE: 89.340775  [12864/58532]\n",
      "MSE: 95.385284  [19264/58532]\n",
      "MSE: 121.887169  [25664/58532]\n",
      "MSE: 60.172108  [32064/58532]\n",
      "MSE: 82.810242  [38464/58532]\n",
      "MSE: 72.753189  [44864/58532]\n",
      "MSE: 73.739510  [51264/58532]\n",
      "MSE: 41.107548  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.28479766845703\n",
      "Validation Error: \n",
      " Avg MSE: 74.168874 \n",
      " Avg MAE: 6.332116 \n",
      " Avg R2: 0.153789\n",
      " Bias: -0.166741\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "MSE: 58.194710  [   64/58532]\n",
      "MSE: 42.205574  [ 6464/58532]\n",
      "MSE: 89.421394  [12864/58532]\n",
      "MSE: 93.854813  [19264/58532]\n",
      "MSE: 122.435097  [25664/58532]\n",
      "MSE: 60.117386  [32064/58532]\n",
      "MSE: 83.560104  [38464/58532]\n",
      "MSE: 72.408684  [44864/58532]\n",
      "MSE: 73.665970  [51264/58532]\n",
      "MSE: 41.499367  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.27335357666016\n",
      "Validation Error: \n",
      " Avg MSE: 74.173974 \n",
      " Avg MAE: 6.335882 \n",
      " Avg R2: 0.153389\n",
      " Bias: -0.157551\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "MSE: 57.977634  [   64/58532]\n",
      "MSE: 42.222462  [ 6464/58532]\n",
      "MSE: 89.165047  [12864/58532]\n",
      "MSE: 94.189308  [19264/58532]\n",
      "MSE: 122.122879  [25664/58532]\n",
      "MSE: 60.318745  [32064/58532]\n",
      "MSE: 83.094673  [38464/58532]\n",
      "MSE: 72.694084  [44864/58532]\n",
      "MSE: 73.769081  [51264/58532]\n",
      "MSE: 40.129562  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.21772766113281\n",
      "Validation Error: \n",
      " Avg MSE: 74.165090 \n",
      " Avg MAE: 6.335275 \n",
      " Avg R2: 0.153550\n",
      " Bias: -0.128855\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "MSE: 58.322121  [   64/58532]\n",
      "MSE: 41.908566  [ 6464/58532]\n",
      "MSE: 88.277336  [12864/58532]\n",
      "MSE: 93.611862  [19264/58532]\n",
      "MSE: 122.419182  [25664/58532]\n",
      "MSE: 60.213703  [32064/58532]\n",
      "MSE: 83.941444  [38464/58532]\n",
      "MSE: 72.692596  [44864/58532]\n",
      "MSE: 73.326492  [51264/58532]\n",
      "MSE: 40.239449  [57664/58532]\n",
      "In questa epoca, l'MSE medio è 75.24711608886719\n",
      "Validation Error: \n",
      " Avg MSE: 74.300956 \n",
      " Avg MAE: 6.354400 \n",
      " Avg R2: 0.151632\n",
      " Bias: -0.068086\n",
      "\n",
      "Early Stop attivato.\n",
      "Fatto!\n"
     ]
    }
   ],
   "source": [
    "import sales_prediction.step.pred_tabular as pt\n",
    "from sales_prediction.sales_prediction import EarlyStopping\n",
    "from sales_prediction.preproc_tabular import get_tabular\n",
    "from torch.optim.adamw import AdamW\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "img_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\ResizedImages\\\\'\n",
    "desc_path= 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\descrizioni\\\\descrizioni.pt'\n",
    "desc_tot_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\descrizioni\\\\descrizioni_tot.pt'\n",
    "\n",
    "tab_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\sales_anagrafica_final.csv'\n",
    "aggregated_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_season.csv'\n",
    "aggregated_month_path = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\aggregated_sales_month.csv'\n",
    "\n",
    "noneg_week = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\sales_anagrafica_final.csv'\n",
    "noneg_month = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\front_img_season.csv'\n",
    "noneg_season = 'C:\\\\Users\\\\GRVRLD00P\\\\Documents\\\\Progetto ORS\\\\Dati\\\\nonegozio_season.csv'\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Sto usando {device}\")\n",
    "\n",
    "modello = pt.create_model(1, 1).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(modello.parameters(), lr=0.01) \n",
    "early_stop = EarlyStopping(7, 0.001) \n",
    "\n",
    "data, _ , _, target = get_tabular(aggregated_path, desc_path)\n",
    "train, val = pt.getDataset(data, target, batch_size, 1)\n",
    "\n",
    "mse_final, mae_final, r2_final = 0, 0, 0\n",
    "modello_final = 0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    pt.train_loop(train, modello, criterion, optimizer, batch_size, device)\n",
    "    mse, mae, r2, bias = pt.validation_loop(val, modello, criterion, device)\n",
    "    early_stp = early_stop(mse)\n",
    "\n",
    "    if early_stp:\n",
    "        print('Early Stop attivato.')\n",
    "        break\n",
    "    \n",
    "print(\"Fatto!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
